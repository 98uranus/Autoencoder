{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, Model\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers, layers, losses\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from keras.utils import plot_model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Generate some sample data for the autoencoder\n",
    "# x_train = np.random.rand(1000, 12)\n",
    "# x_test = np.random.rand(100, 12)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "df = pd.read_csv(\"Autoencoder1.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from cmath import pi\n",
    "\n",
    "data = df.drop(['Time'], axis=1)\n",
    "y_true = df['Class']\n",
    "\n",
    "r2d = 180/pi\n",
    "\n",
    "data['P'] = r2d * data['P']\n",
    "data['Q'] = r2d * data['Q']\n",
    "data['R'] = r2d * data['R']\n",
    "data['Phi'] = r2d * data['Phi']\n",
    "data['Theta'] = r2d * data['Theta']\n",
    "data['Psi'] = r2d * data['Psi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.3, random_state=RANDOM_SEED)\n",
    "# X_train = X_train[X_train.Class == 0]\n",
    "y_train = X_train['Class']\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train = X_train.drop(['Class'], axis=1)\n",
    "\n",
    "# X_test = X_test[X_test.Class == 0]\n",
    "y_test = X_test['Class']\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test = X_test.drop(['Class'], axis=1)\n",
    "\n",
    "def normalize(label):\n",
    "    str = StandardScaler()\n",
    "    str.fit(X_train[label].values.reshape(-1, 1))\n",
    "    X_train[label] = str.transform(X_train[label].values.reshape(-1, 1))\n",
    "    X_test[label] = str.transform(X_test[label].values.reshape(-1, 1))\n",
    "\n",
    "def normalize1(label):\n",
    "    str = MinMaxScaler()\n",
    "    str.fit(X_train[label].values.reshape(-1, 1))\n",
    "    X_train[label] = str.transform(X_train[label].values.reshape(-1, 1))\n",
    "    X_test[label] = str.transform(X_test[label].values.reshape(-1, 1))\n",
    "\n",
    "A = ['RPM1', 'RPM2', 'RPM3', 'RPM4', 'RPM5', 'RPM6', 'P', 'Q', 'R', 'Phi', 'Theta', 'Psi']\n",
    "\n",
    "for i in A:\n",
    "    normalize1(i)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 12\n",
    "latent_dim = 3\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(8, activation='sigmoid',\n",
    "                        activity_regularizer=regularizers.l2(10e-3)),\n",
    "            layers.Dense(4, activation='relu'),\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(8, activation='relu'),\n",
    "            layers.Dense(input_dim, activation='tanh'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(latent_dim)\n",
    "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "autoencoder = Autoencoder(latent_dim)\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "checkpointer = ModelCheckpoint(filepath=\"DAGMM\",\n",
    "                               monitor='val_loss',\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "batch_size = 4\n",
    "nb_epochs = 5\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                        epochs=nb_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_test, X_test),\n",
    "                        callbacks=[checkpointer, tensorboard]).history\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "plt.plot(range(nb_epochs), history['loss'], label='Training Loss')\n",
    "plt.plot(range(nb_epochs), history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input layer for the autoencoder\n",
    "# input_layer = Input(shape=(12,))\n",
    "\n",
    "# # Define the encoder layers\n",
    "# encoded = layers.Dense(8, activation='sigmoid')(input_layer)\n",
    "# encoded = layers.Dense(4, activation='relu')(encoded)\n",
    "\n",
    "# # Define the decoder layers\n",
    "# decoded = layers.Dense(8, activation='relu')(encoded)\n",
    "# decoded = layers.Dense(12, activation='tanh')(decoded)\n",
    "\n",
    "# # Create the autoencoder model\n",
    "# autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Plot the model structure\n",
    "# plot_model(autoencoder, to_file='DAGMM.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=32, validation_data=(x_test, x_test))\n",
    "\n",
    "# Plot the model loss\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained autoencoder to encode the training data\n",
    "encoded_data = autoencoder.encoder(X_train).np()\n",
    "\n",
    "# Fit a Gaussian Mixture Model with 2 components to the encoded training data\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GMM to predict the probability densities for the encoded test data\n",
    "encoded_test_data = autoencoder.encoder(X_test).np()\n",
    "scores = gmm.score_samples(encoded_test_data)\n",
    "\n",
    "# Identify anomalies based on the GMM probability densities\n",
    "threshold = np.percentile(scores, 5)\n",
    "anomalies = X_test[scores < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision, recall, and F1 score\n",
    "y_true = np.zeros_like(scores)\n",
    "y_true[scores < threshold] = 1\n",
    "y_pred = np.zeros_like(scores)\n",
    "y_pred[scores < threshold] = 1\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 score: {f1:.4f}\")\n",
    "\n",
    "# Generate precision-recall curve with F1 score\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, scores)\n",
    "f1_scores = 2 * precision * recall / (precision + recall)\n",
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.plot(recall, f1_scores, label='F1 score')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision/F1 score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate score with threshold graph\n",
    "plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbe8294542bfef95dc4c1617ea160a86e04009fef3feaf3f675181605b6be6e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
